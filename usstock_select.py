#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import pytz
import json
from datetime import datetime, timedelta
import numpy as np
from tqdm import tqdm
import yfinance as yf
import os
import time
import pandas as pd
import numpy as np
import yfinance as yf
from concurrent.futures import ThreadPoolExecutor, as_completed

# ğŸ”„ åˆ‡æ¢åˆ°6783è‚¡ç¥¨æ± 
file_path = os.path.join(os.getcwd(), 'usstock_all.txt')  # ä» usstock.txt æ”¹ä¸º usstock_all.txt
save_dir = os.path.join(os.getcwd(), 'result')
cache_dir = os.path.join(os.getcwd(), 'cache')  # æ•°æ®ç¼“å­˜ç›®å½•


def read_stock_list(file_path):
    try:
        # 1. è¯»å–æ–‡ä»¶
        df = pd.read_csv(file_path, encoding='utf-8')
        
        bp500_list = df['ä»£ç '].tolist()
        
        print(f"æˆåŠŸè¯»å– {len(bp500_list)} ä¸ªè‚¡ç¥¨ä»£ç ")
        print(bp500_list)
        return bp500_list

    except Exception as e:
        print(f"è¯»å–å‡ºé”™: {e}")
        return None



def _apply_proxy(use_proxy=False, http_proxy=None, https_proxy=None):
    if use_proxy:
        if http_proxy:
            os.environ["HTTP_PROXY"] = http_proxy
            os.environ["http_proxy"] = http_proxy
        if https_proxy:
            os.environ["HTTPS_PROXY"] = https_proxy
            os.environ["https_proxy"] = https_proxy
    else:
        for k in ["HTTP_PROXY","http_proxy","HTTPS_PROXY","https_proxy"]:
            os.environ.pop(k, None)


def _to_naive_datetime(idx):
    dt = pd.to_datetime(idx)
    if hasattr(dt, "tz") and dt.tz is not None:
        try:
            dt = dt.tz_convert(None)
        except Exception:
            try:
                dt = dt.tz_localize(None)
            except Exception:
                pass
    return dt


def _safe_get_info(tk: yf.Ticker):
    try:
        return tk.get_info() or {}
    except Exception:
        return {}


def _get_shares_history_df(tk: yf.Ticker):
    """
    è¿”å› DataFrame(index: æŠ«éœ²æ—¥, col: shares_outstanding)ï¼Œè‹¥æ— åˆ™ None
    """
    try:
        sh = tk.get_shares_full()
    except Exception:
        sh = None
    if isinstance(sh, pd.DataFrame) and not sh.empty:
        sh = sh.copy()
        sh.index = _to_naive_datetime(sh.index)
        if "Shares" in sh.columns:
            sh = sh[["Shares"]].rename(columns={"Shares": "shares_outstanding"})
        elif "shares_outstanding" in sh.columns:
            sh = sh[["shares_outstanding"]]
        else:
            return None
        sh = sh.sort_index()
        return sh
    return None


def _normalize_ratio(x):
    """
    å°† x å½’ä¸€åŒ–ä¸º [0,1] æ¯”ä¾‹:
    - è‹¥ > 1.5ï¼ˆä¾‹å¦‚ 70 è¡¨ç¤º 70%ï¼‰ï¼Œåˆ™é™¤ä»¥ 100
    - è‹¥æ˜¯ None/NaN è¿”å› NaN
    """
    try:
        if x is None:
            return np.nan
        v = float(x)
        if np.isnan(v):
            return np.nan
        return v/100.0 if v > 1.5 else v
    except Exception:
        return np.nan


def fetch_one(
    sym,
    start=None,
    end=None,
    use_proxy=False,
    http_proxy=None,
    https_proxy=None
):
    # ç¡®ä¿ä»£ç†ç”Ÿæ•ˆ
    _apply_proxy(use_proxy, http_proxy, https_proxy)

    tk = yf.Ticker(sym)

    # ä»·æ ¼å†å²
    px = tk.history(start=start, end=end, auto_adjust=False)
    if px is None or px.empty:
        return None
    px = px.rename(columns=str.lower)
    need_cols = ["open","high","low","close","volume","country"]
    px = px[[c for c in need_cols if c in px.columns]]
    px.index = _to_naive_datetime(px.index)
    px = px.sort_index()

    # ä½é¢‘ä¿¡æ¯ï¼šè¡Œä¸š/æ¿å—ã€float/shares å¿«ç…§ + æœºæ„/å†…éƒ¨æŒè‚¡æ¯”ä¾‹
    sector = None
    industry = None
    country = None  # æ–°å¢ï¼šå…¬å¸æ‰€åœ¨å›½å®¶
    shares_out_snap = None
    float_snap = None

    # æ–°å¢ï¼šæœºæ„ä¸å†…éƒ¨æŒè‚¡æ¯”ä¾‹ï¼ˆç›®æ ‡ç»Ÿä¸€åˆ° 0-1ï¼‰
    institution_pct = np.nan
    insider_pct = np.nan

    info = _safe_get_info(tk)
    if isinstance(info, dict):
        sector = info.get("sector")
        industry = info.get("industry") or info.get("industryKey") or info.get("industryDisp")
        country = info.get("country")  # è·å–å…¬å¸æ‰€åœ¨å›½å®¶
        shares_out_snap = info.get("sharesOutstanding")
        float_snap = info.get("floatShares") or info.get("float") or info.get("float_shares")

        # å¸¸è§é”®å…¼å®¹ï¼šinstitutionPercent/institutionsPercent/heldPercentInstitutions
        inst_keys = [
            "institutionPercent",
            "institutionsPercent",
            "heldPercentInstitutions",
            "institutionOwnership"
        ]
        for k in inst_keys:
            if k in info and pd.notna(info[k]):
                institution_pct = _normalize_ratio(info[k])
                break

        # å†…éƒ¨æŒè‚¡æ¯”ä¾‹ï¼šheldPercentInsiders/insiderOwnership
        insider_keys = [
            "heldPercentInsiders",
            "insiderOwnership",
            "insidersPercent"
        ]
        for k in insider_keys:
            if k in info and pd.notna(info[k]):
                insider_pct = _normalize_ratio(info[k])
                break

    # fast_info å…œåº•
    try:
        fi = getattr(tk, "fast_info", None)
    except Exception:
        fi = None
    if fi is not None:
        try:
            sector = sector or getattr(fi, "sector", None)
        except Exception:
            pass
        try:
            industry = industry or getattr(fi, "industry", None)
        except Exception:
            pass
        try:
            float_snap = float_snap or getattr(fi, "float_shares", None)
        except Exception:
            pass
        # æŸäº›ç‰ˆæœ¬ fast_info ä¹Ÿå¯èƒ½æœ‰ held_percent_* å­—æ®µ
        for attr in ["held_percent_institutions", "institution_percent", "institutions_percent"]:
            try:
                v = getattr(fi, attr, None)
                if pd.notna(v) and np.isnan(institution_pct):
                    institution_pct = _normalize_ratio(v)
            except Exception:
                pass
        for attr in ["held_percent_insiders", "insider_percent", "insiders_percent"]:
            try:
                v = getattr(fi, attr, None)
                if pd.notna(v) and np.isnan(insider_pct):
                    insider_pct = _normalize_ratio(v)
            except Exception:
                pass

    # å­£åº¦ shares å†å²ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
    shares_hist = _get_shares_history_df(tk)

    # æ„é€ â€œæ¯æ—¥ float_sharesâ€å¹¶ä»…å‘å‰å¡«å……
    df = px.copy()
    df["symbol"] = sym
    df = df.reset_index().rename(columns={"index":"date", "Date":"date"})
    if "date" not in df.columns:
        df.insert(0, "date", _to_naive_datetime(px.index))
    df["date"] = pd.to_datetime(df["date"]).dt.tz_localize(None)

    float_daily = None
    if shares_hist is not None and not shares_hist.empty:
        rhs = shares_hist.reset_index().rename(columns={"index":"date"})
        rhs["date"] = pd.to_datetime(rhs["date"]).dt.tz_localize(None)
        rhs = rhs.sort_values("date")

        merged = pd.merge_asof(
            left=df[["date"]].sort_values("date"),
            right=rhs[["date","shares_outstanding"]],
            on="date",
            direction="backward",
            allow_exact_matches=True
        )
        float_daily = merged["shares_outstanding"].astype("float64")

    # å¦‚æœå†å²ç¼ºå¤±æˆ–å…¨ç©ºï¼Œä½¿ç”¨å¿«ç…§ï¼ˆfloat_snap ä¼˜å…ˆï¼Œå…¶æ¬¡ shares_out_snapï¼‰
    if float_daily is None or float_daily.isna().all():
        seed = np.nan
        if pd.notna(float_snap):
            seed = float(float_snap)
        elif pd.notna(shares_out_snap):
            seed = float(shares_out_snap)
        float_daily = pd.Series(seed, index=df.index, dtype="float64")

    # ä»…å‰å‘å¡«å……ï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
    float_daily = float_daily.ffill()
    df["float_shares"] = float_daily.values

    # è®¡ç®—
    df["turnover_value"] = df.get("close") * df.get("volume")
    # æ¢æ‰‹ç‡ï¼šæˆäº¤é‡ / æµé€šè‚¡æœ¬
    df["turnover_rate"] = np.where(
        (df["float_shares"].notna()) & (df["float_shares"] > 0),
        df["volume"] / df["float_shares"],
        np.nan
    )
    # è‹¥ä½¿ç”¨æˆäº¤é¢/æµé€šå¸‚å€¼ï¼š
    # df["turnover_rate"] = df["turnover_value"] / (df["close"] * df["float_shares"])

    df["float_mktcap"] = np.where(
        df["float_shares"].notna(),
        df["close"] * df["float_shares"],
        np.nan
    )

    # è¡Œä¸š/æ¿å— + æœºæ„/å†…éƒ¨æŒè‚¡ + å›½å®¶ï¼ˆå¿«ç…§å¹¿æ’­ï¼‰
    df["sector"] = sector
    df["industry"] = industry
    df["country"] = country  # æ–°å¢ï¼šå…¬å¸æ‰€åœ¨å›½å®¶
    df["institution_pct"] = institution_pct
    df["insider_pct"] = insider_pct

    keep = [
        "symbol","open","high","low","close","volume",
        "turnover_value","float_mktcap","turnover_rate",
        "sector","industry","institution_pct","insider_pct",
        "date","country"
    ]
    df = df[keep]
    df["date"] = pd.to_datetime(df["date"]).dt.date
    df = df.set_index("date")
    return df


def yahoo_datas(
    symbols,
    n_days=252,
    use_proxy=False,
    http_proxy=None,
    https_proxy=None,
    max_workers=8,
    batch_size=500,      # æ¯æ‰¹è·å–çš„è‚¡ç¥¨æ•°é‡
    batch_pause=300,     # æ¯æ‰¹ä¹‹é—´æš‚åœçš„ç§’æ•°ï¼ˆ5åˆ†é’Ÿï¼‰
    use_cache=True,      # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    force_refresh=False  # æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
):
    """
    åˆ†æ‰¹è·å–è‚¡ç¥¨æ•°æ®ï¼Œé¿å…è¢« Yahoo Finance é™æµã€‚
    æ¯è·å– batch_size åªè‚¡ç¥¨åæš‚åœ batch_pause ç§’ã€‚
    
    å‚æ•°:
        use_cache: æ˜¯å¦ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼Œå¿½ç•¥å·²æœ‰æ•°æ®ï¼ˆé»˜è®¤Falseï¼‰
    """
    # ç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼ˆåŸºäºæ—¥æœŸå’Œè‚¡ç¥¨æ•°é‡ï¼‰
    today_str = datetime.now().strftime("%Y-%m-%d")
    cache_filename = f"stock_data_{today_str}_{len(symbols)}stocks_{n_days}days.pkl"
    cache_filepath = os.path.join(cache_dir, cache_filename)
    
    # å°è¯•ä»ç¼“å­˜åŠ è½½
    if use_cache and not force_refresh and os.path.exists(cache_filepath):
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ å‘ç°ç¼“å­˜æ–‡ä»¶: {cache_filename}")
        print(f"{'='*60}")
        try:
            cached_data = pd.read_pickle(cache_filepath)
            print(f"âœ… æˆåŠŸä»ç¼“å­˜åŠ è½½ {len(cached_data.index.get_level_values('symbol').unique())} åªè‚¡ç¥¨æ•°æ®")
            print(f"   æ•°æ®æ—¶é—´èŒƒå›´: {cached_data.index.get_level_values('time').min()} è‡³ {cached_data.index.get_level_values('time').max()}")
            print(f"{'='*60}\n")
            return cached_data
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            print(f"   å°†é‡æ–°è·å–æ•°æ®...\n")
    
    # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œåˆ™ä»APIè·å–
    if force_refresh:
        print(f"\n{'='*60}")
        print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼šå¿½ç•¥ç¼“å­˜ï¼Œé‡æ–°è·å–æ•°æ®")
        print(f"{'='*60}\n")
    
    # è®¡ç®—èµ·æ­¢æ—¥æœŸï¼šå–æœ€è¿‘ n_days ä¸ªè‡ªç„¶æ—¥çš„å¼€å§‹ï¼Œäº¤ç»™ yfinance è‡ªå·±åšäº¤æ˜“æ—¥ç­›é€‰
    end_ts = pd.Timestamp.today().normalize()
    start_ts = end_ts - pd.Timedelta(days=int(n_days*2))  # æ”¾å®½çª—å£ï¼Œé˜²æ­¢éäº¤æ˜“æ—¥ä¸è¶³
    start = start_ts.strftime("%Y-%m-%d")
    end = None  # åˆ°ä»Šå¤©

    results = []
    total_symbols = len(symbols)
    
    # å°†è‚¡ç¥¨åˆ—è¡¨åˆ†æˆå¤šä¸ªæ‰¹æ¬¡
    batches = [symbols[i:i + batch_size] for i in range(0, total_symbols, batch_size)]
    total_batches = len(batches)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å¼€å§‹åˆ†æ‰¹è·å–æ•°æ®")
    print(f"   æ€»è‚¡ç¥¨æ•°: {total_symbols}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    print(f"   æ‰¹æ¬¡é—´éš”: {batch_pause} ç§’ ({batch_pause//60} åˆ†é’Ÿ)")
    print(f"{'='*60}\n")
    
    for batch_idx, batch_symbols in enumerate(batches, 1):
        batch_start_time = time.time()
        print(f"\nğŸ“¦ æ­£åœ¨è·å–ç¬¬ {batch_idx}/{total_batches} æ‰¹ ({len(batch_symbols)} åªè‚¡ç¥¨)...")
        
        batch_results = []
        success_count = 0
        fail_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futs = {
                ex.submit(
                    fetch_one,
                    s,
                    start,
                    end,
                    use_proxy,
                    http_proxy,
                    https_proxy
                ): s for s in batch_symbols
            }
            for fut in as_completed(futs):
                sym = futs[fut]
                try:
                    df = fut.result()
                    if df is not None and not df.empty:
                        df["symbol"] = sym
                        batch_results.append(df)
                        success_count += 1
                        print(f"  âœ… [{sym}] è·å– {len(df)} è¡Œæ•°æ®")
                    else:
                        fail_count += 1
                        print(f"  âš ï¸ [{sym}] æ— æ•°æ®")
                except Exception as e:
                    fail_count += 1
                    print(f"  âŒ [{sym}] é”™è¯¯: {e}")
        
        results.extend(batch_results)
        batch_elapsed = time.time() - batch_start_time
        
        print(f"\nğŸ“Š ç¬¬ {batch_idx} æ‰¹å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}, è€—æ—¶ {batch_elapsed:.1f}ç§’")
        print(f"   ç´¯è®¡è·å–: {len(results)} åªè‚¡ç¥¨")
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œæš‚åœç­‰å¾…
        if batch_idx < total_batches:
            print(f"\nâ³ æš‚åœ {batch_pause} ç§’ ({batch_pause//60} åˆ†é’Ÿ) ä»¥é¿å…é™æµ...")
            for remaining in range(batch_pause, 0, -30):
                print(f"   å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining} ç§’...")
                time.sleep(min(30, remaining))
            print("   âœ… ç»§ç»­è·å–ä¸‹ä¸€æ‰¹...")

    if not results:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•è‚¡ç¥¨æ•°æ®ï¼")
        return pd.DataFrame()

    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨è·å–å®Œæˆï¼å…±è·å– {len(results)} åªè‚¡ç¥¨æ•°æ®")
    print(f"{'='*60}\n")

    out = pd.concat(results).sort_index()
    # åªä¿ç•™æœ€è¿‘ n_days ä¸ªâ€œäº¤æ˜“æ—¥â€ï¼ˆæ¯ä¸ª symbol è‡ªå·±çš„å°¾éƒ¨ n å¤©ï¼‰
    out = out.groupby("symbol", group_keys=False).apply(lambda x: x.tail(n_days))
    out.index.name = "time"
    out = out.set_index("symbol", append=True)

    # è¡Œä¸š/æ¿å—/æœºæ„/å†…éƒ¨æŒè‚¡å…œåº•ï¼ˆtransform é¿å… index çº§åˆ«é”™é…ï¼‰
    for col in ["sector", "industry", "institution_pct", "insider_pct"]:
        if col in out.columns:
            s = out[col]
            filled = (
                s.groupby(level=1)
                 .transform(lambda x: x.ffill().bfill())
            )
            out[col] = filled

    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache:
        try:
            os.makedirs(cache_dir, exist_ok=True)
            # ä½¿ç”¨ Pickle æ ¼å¼ä¿å­˜ï¼ˆæ›´ç¨³å®šï¼Œå…¼å®¹æ€§å¥½ï¼‰
            out.to_pickle(cache_filepath)
            file_size_mb = os.path.getsize(cache_filepath) / (1024 * 1024)
            print(f"\n{'='*60}")
            print(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜åˆ°æœ¬åœ°")
            print(f"   æ–‡ä»¶: {cache_filename}")
            print(f"   å¤§å°: {file_size_mb:.2f} MB")
            print(f"   ä½ç½®: {cache_dir}")
            print(f"{'='*60}\n")
        except Exception as e:
            print(f"\nâš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}\n")

    return out


def get_data_single_stock_with_cache(symbol: str, n_days: int = 252) -> pd.DataFrame:
    """
    ä»ç¼“å­˜ä¸­è·å–å•åªè‚¡ç¥¨çš„æ•°æ®ï¼Œç”¨äºå›æµ‹ã€‚
    
    å‚æ•°:
        symbol: è‚¡ç¥¨ä»£ç 
        n_days: æ•°æ®å¤©æ•°
        
    è¿”å›:
        DataFrameï¼ŒåŒ…å«è¯¥è‚¡ç¥¨çš„OHLCVæ•°æ®ï¼Œç´¢å¼•ä¸ºæ—¥æœŸ
    """
    import glob
    
    # æŸ¥æ‰¾æœ€æ–°çš„ç¼“å­˜æ–‡ä»¶
    cache_files = glob.glob(os.path.join(cache_dir, "stock_data_*.pkl"))
    if not cache_files:
        print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œæ— æ³•è·å– {symbol} æ•°æ®")
        return pd.DataFrame()
    
    # ä½¿ç”¨æœ€æ–°çš„ç¼“å­˜æ–‡ä»¶
    latest_cache = sorted(cache_files)[-1]
    
    try:
        all_data = pd.read_pickle(latest_cache)
        
        # æ£€æŸ¥ symbol æ˜¯å¦å­˜åœ¨
        symbols_in_cache = all_data.index.get_level_values("symbol").unique()
        if symbol not in symbols_in_cache:
            print(f"âš ï¸ ç¼“å­˜ä¸­æ²¡æœ‰ {symbol} çš„æ•°æ®")
            return pd.DataFrame()
        
        # æå–è¯¥è‚¡ç¥¨çš„æ•°æ®
        stock_data = all_data.xs(symbol, level="symbol").copy()
        stock_data = stock_data.sort_index()
        
        # åªä¿ç•™æœ€è¿‘ n_days å¤©
        stock_data = stock_data.tail(n_days)
        
        return stock_data
        
    except Exception as e:
        print(f"âŒ è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        return pd.DataFrame()


def build_base_universe(datas: pd.DataFrame) -> pd.DataFrame:
    """
    æ„å»ºåŸºç¡€é€‰è‚¡æ±  ï¼š
    ç°åœ¨è¿”å›çš„ DataFrame ç´¢å¼•ä¸º MultiIndex (time, symbol)ã€‚
    """
    # 1. æ‹·è´å¹¶ç¡®ä¿ç´¢å¼•æœ‰åº (time, symbol)
    df = datas.copy()
    
    # ç´¢å¼•æ ‡å‡†åŒ–å¤„ç†
    if "symbol" in df.columns and "time" not in df.index.names:
        if "time" in df.columns:
            df = df.set_index(["time", "symbol"])
    
    # å¼ºåˆ¶æ’åºï¼Œè¿™å¯¹ rolling/ewm è®¡ç®—è‡³å…³é‡è¦
    df = df.sort_index()

    # -------------------------------------------------------------------------
    # 2. è®¡ç®—æŒ‡æ ‡ (ä½¿ç”¨ transform ä¿æŒç´¢å¼•å¯¹é½)
    # -------------------------------------------------------------------------
    
    # 30æ—¥å¹³å‡æˆäº¤é¢
    df["avg_turnover_30"] = (
        df.groupby(level="symbol")["turnover_value"]
        .transform(lambda x: x.rolling(window=30, min_periods=20).mean())
    )
    
    # 30æ—¥å¹³å‡æ¢æ‰‹ç‡
    df["avg_turnover_rate_30"] = (
        df.groupby(level="symbol")["turnover_rate"]
        .transform(lambda x: x.rolling(window=30, min_periods=20).mean())
    )
    
    # EMA 50 & 150
    df["ema50"] = (
        df.groupby(level="symbol")["close"]
        .transform(lambda x: x.ewm(span=50, adjust=False, min_periods=40).mean())
    )
    
    df["ema150"] = (
        df.groupby(level="symbol")["close"]
        .transform(lambda x: x.ewm(span=150, adjust=False, min_periods=120).mean())
    )

    # -------------------------------------------------------------------------
    # 3. æˆªå–æœ€åä¸€å¤©è¿›è¡Œé€‰è‚¡ (Snapshot) - ã€ä¿®å¤ç‚¹ã€‘
    # -------------------------------------------------------------------------
    last_day = df.index.get_level_values("time").max()
    
    # ã€ä¿®æ”¹å‰ã€‘ snap = df.xs(last_day, level="time").copy() -> ä¼šä¸¢å¤± time ç´¢å¼•
    # ã€ä¿®æ”¹åã€‘ ä½¿ç”¨å¸ƒå°”ç´¢å¼•ï¼Œæˆ–è€… xs(..., drop_level=False)
    # è¿™é‡Œä½¿ç”¨å¸ƒå°”ç´¢å¼•ï¼Œç¡®ä¿ç»“æœä¾ç„¶æ˜¯ MultiIndex: (time, symbol)
    snap = df[df.index.get_level_values("time") == last_day].copy()
    
    # -------------------------------------------------------------------------
    # 4. æ‰§è¡Œç­›é€‰é€»è¾‘
    # -------------------------------------------------------------------------

    # --- 0. å›½å®¶è¿‡æ»¤ï¼ˆåªä¿ç•™ç¾å›½å…¬å¸ï¼‰---
    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ country å­—æ®µï¼Œåˆ™é»˜è®¤å…¨éƒ¨é€šè¿‡ï¼ˆå…¼å®¹æ—§ç¼“å­˜ï¼‰
    if "country" in snap.columns:
        country_ok = snap["country"].astype(str).str.strip().str.lower() == "united states"
    else:
        country_ok = pd.Series(True, index=snap.index)  # æ—§ç¼“å­˜æ—  countryï¼Œé»˜è®¤é€šè¿‡
        snap["country"] = None  # æ·»åŠ ç©ºçš„ country åˆ—

    # --- A. åŸºç¡€é—¨æ§› ---
    mktcap_ok = snap["float_mktcap"] >= 30_000_000
    avg_turnover_ok = snap["avg_turnover_30"] >= 10_000_000
    price_ok = snap["close"] > 1.0

    # --- B. è¡Œä¸šé»‘åå• ---
    exclude_keywords = [
        "Biotech", "Bio-tech", 
        "Healthcare", "Health Care", 
        "Regional Bank", "Banks - Regional", 
        "Shell Company", "Blank Check", "SPAC", 
        "REIT", "Real Estate",
        # ç©ºå£³å…¬å¸ç±»å‹
        "Shell Companies", "Acquisition", "Special Purpose",
        "Merger", "Holdings Company"
    ]
    
    s_sector = snap["sector"].astype(str).fillna("").str.lower()
    s_industry = snap["industry"].astype(str).fillna("").str.lower()
    
    def is_blacklisted(series, keywords):
        mask = pd.Series(False, index=series.index)
        for k in keywords:
            mask |= series.str.contains(k.lower(), regex=False)
        return mask

    is_excluded = is_blacklisted(s_sector, exclude_keywords) | \
                  is_blacklisted(s_industry, exclude_keywords)
    
    sector_ok = ~is_excluded

    # --- C. æŒä»“ç»“æ„ ---
    def normalize_pct(s):
        """
        å°†æ¯”ä¾‹å½’ä¸€åŒ–åˆ° [0, 1] åŒºé—´
        ä¿®å¤ï¼šæ”¹ä¸ºé€å…ƒç´ åˆ¤æ–­ï¼Œè€Œä¸æ˜¯æ•´åˆ—åˆ¤æ–­
        - å¦‚æœå€¼ > 1.5 (ä¾‹å¦‚ 70 è¡¨ç¤º 70%)ï¼Œåˆ™é™¤ä»¥ 100
        - å¦‚æœå€¼ <= 1.5 (ä¾‹å¦‚ 0.7 è¡¨ç¤º 70%)ï¼Œåˆ™ä¿æŒä¸å˜
        """
        s = pd.to_numeric(s, errors='coerce')
        # é€å…ƒç´ åˆ¤æ–­ï¼šå€¼ > 1.5 çš„æ‰é™¤ä»¥ 100
        return s.apply(lambda x: x / 100.0 if pd.notna(x) and x > 1.5 else x)

    inst_ratio = normalize_pct(snap.get("institution_pct", pd.Series(0, index=snap.index)))
    inst_ok = inst_ratio < 0.70

    insider_ratio = normalize_pct(snap.get("insider_pct", pd.Series(0, index=snap.index)))
    insider_ok = insider_ratio < 0.30
    
    # å¦‚æœ30æ—¥å¹³å‡æ¢æ‰‹ç‡é«˜äº2%ï¼Œå¿½ç•¥æœºæ„æŒè‚¡å’Œå†…éƒ¨æŒè‚¡æ¡ä»¶
    high_turnover = snap["avg_turnover_rate_30"] > 0.02
    structure_ok = (inst_ok & insider_ok) | high_turnover

    # --- D. æŠ€æœ¯å½¢æ€ ---
    ema_ok = ((snap["close"] > snap["ema50"]) & (snap["ema50"] > snap["ema150"])).fillna(False)

    # --- E. æ¢æ‰‹ç‡æ´»è·ƒåº¦ ---
    # æ¢æ‰‹ç‡ä½¿ç”¨åŸå§‹å€¼ï¼ˆå·²ç»æ˜¯å°æ•°å½¢å¼ï¼Œå¦‚ 0.035 è¡¨ç¤º 3.5%ï¼‰
    # ä¸éœ€è¦ normalize_pctï¼Œå› ä¸ºæ¢æ‰‹ç‡ä¸ä¼šå‡ºç° "70è¡¨ç¤º70%" è¿™ç§æƒ…å†µ
    tr_30 = snap["avg_turnover_rate_30"]
    turnover_rate_ok = tr_30 > 0.01

    # -------------------------------------------------------------------------
    # 5. æ±‡æ€»ç»“æœ
    # -------------------------------------------------------------------------
    snap["cond_country"] = country_ok  # æ–°å¢ï¼šå›½å®¶æ¡ä»¶
    snap["cond_mktcap"] = mktcap_ok
    snap["cond_liq"] = avg_turnover_ok
    snap["cond_price"] = price_ok
    snap["cond_sector"] = sector_ok
    snap["cond_structure"] = structure_ok
    snap["cond_trend"] = ema_ok
    snap["cond_activity"] = turnover_rate_ok

    cond_cols = [
        "cond_country",  # å›½å®¶æ¡ä»¶
        "cond_mktcap", "cond_liq", "cond_price", 
        "cond_sector", "cond_structure", 
        "cond_trend", "cond_activity"
    ]
    snap["in_pool"] = snap[cond_cols].all(axis=1)

    output_cols = [
        "close", "volume", "float_mktcap", "turnover_value",
        "sector", "industry", "country",
        "avg_turnover_30", "avg_turnover_rate_30",
        "ema50", "ema150",
        "institution_pct", "insider_pct",
        "in_pool"
    ] + cond_cols
    
    return snap[output_cols].sort_values(["in_pool", "float_mktcap"], ascending=[False, False])


def add_derived_features(datas: pd.DataFrame) -> pd.DataFrame:
    """
    åœ¨åŸå§‹ datas ä¸Šè¡¥å……æŠ€æœ¯æŒ‡æ ‡ï¼Œå…¨ç¨‹ä½¿ç”¨ transform ä¿æŒç´¢å¼•å¯¹é½ï¼Œ
    é¿å… reset_index å¯¼è‡´çš„æ½œåœ¨é”™ä½ã€‚
    """
    df = datas.copy()

    # 1. ç¡®ä¿ç´¢å¼•æ˜¯ (time, symbol) ä¸”æ’åº
    if "symbol" in df.columns and "time" in df.columns:
        df = df.set_index(["time", "symbol"])
    
    if df.index.names != ["time", "symbol"]:
        # å°è¯•è‡ªåŠ¨ä¿®å¤
        if "symbol" in df.columns:
            df = df.reset_index().set_index(["time", "symbol"])
    
    df = df.sort_index()

    # 2. åŸºç¡€è®¡ç®— (ä½¿ç”¨ transform æ•ˆç‡æ›´é«˜ä¸”å®‰å…¨)
    # å‡çº¿ (ä½¿ç”¨ EMA æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œå¯¹ä»·æ ¼å˜åŒ–ååº”æ›´æ•æ„Ÿ)
    df["ma5"] = df.groupby(level="symbol")["close"].transform(lambda x: x.ewm(span=5, adjust=False, min_periods=3).mean())
    df["ma10"] = df.groupby(level="symbol")["close"].transform(lambda x: x.ewm(span=10, adjust=False, min_periods=5).mean())
    df["ma20"] = df.groupby(level="symbol")["close"].transform(lambda x: x.ewm(span=20, adjust=False, min_periods=10).mean())
    df["ma50"] = df.groupby(level="symbol")["close"].transform(lambda x: x.ewm(span=50, adjust=False, min_periods=25).mean())

    # é‡æ¯”åˆ†æ¯ï¼š5æ—¥å‡é‡
    df["vol_ma5"] = df.groupby(level="symbol")["volume"].transform(lambda x: x.rolling(5, min_periods=3).mean())

    # æ¢æ‰‹ç‡ (åŸºäºæˆäº¤é¢ / æµé€šå¸‚å€¼)
    # æ³¨æ„ï¼šfloat_mktcap å¯èƒ½ä¸º0æˆ–NaNï¼Œéœ€å¤„ç†
    denom = df["float_mktcap"].replace(0, np.nan)
    df["tr_value"] = df["turnover_value"] / denom

    # 30æ—¥å¹³å‡æ¢æ‰‹ç‡
    df["avg_tr_value_30"] = df.groupby(level="symbol")["tr_value"].transform(lambda x: x.rolling(30, min_periods=15).mean())

    # 60æ—¥æŒ¯å¹…: (High_60 / Low_60) - 1
    roll_high_60 = df.groupby(level="symbol")["high"].transform(lambda x: x.rolling(60, min_periods=30).max())
    roll_low_60 = df.groupby(level="symbol")["low"].transform(lambda x: x.rolling(60, min_periods=30).min())
    df["swing_60"] = (roll_high_60 / roll_low_60) - 1.0
    
    # 60æ—¥æœ€é«˜ä»· (ç”¨äºçªç ´ç­–ç•¥)
    df["high_60"] = roll_high_60
    
    # æ’é™¤æœ€è¿‘10å¤©çš„å†å²æœ€é«˜ç‚¹ (ç”¨äºçªç ´ç­–ç•¥)
    # é€»è¾‘ï¼šä½¿ç”¨ expanding() æ‰¾åˆ°10å¤©å‰åŠæ›´æ—©çš„æ‰€æœ‰å†å²æœ€é«˜ç‚¹
    # shift(10) ç¡®ä¿æ’é™¤æœ€è¿‘10å¤©ï¼Œexpanding() ç¡®ä¿å–åˆ°çœŸæ­£çš„å†å²æœ€é«˜ç‚¹
    df["high_60_ex10"] = df.groupby(level="symbol")["high"].transform(
        lambda x: x.shift(10).expanding(min_periods=1).max()
    )

    # 52å‘¨æœ€é«˜ä»· (252æ—¥)
    df["high_252"] = df.groupby(level="symbol")["high"].transform(lambda x: x.rolling(252, min_periods=60).max())

    # è¾…åŠ©ï¼šå‰ä¸€æ—¥æˆäº¤é¢ (ç”¨äºä½å¸é€»è¾‘) - ä¿ç•™å…¼å®¹
    df["turnover_value_prev"] = df.groupby(level="symbol")["turnover_value"].shift(1)
    
    # è¾…åŠ©ï¼šå‰ä¸€æ—¥æˆäº¤é‡ (ç”¨äºç¼©é‡åˆ¤æ–­ï¼Œæ¯” turnover_value æ›´å‡†ç¡®)
    df["volume_prev"] = df.groupby(level="symbol")["volume"].shift(1)
    
    # è¾…åŠ©ï¼šå‰ä¸€æ—¥æ”¶ç›˜ä»· (ç”¨äºè®¡ç®—æ¶¨å¹…)
    df["close_prev"] = df.groupby(level="symbol")["close"].shift(1)
    
    # è¾…åŠ©ï¼šå‰ä¸€æ—¥çš„å‰ä¸€æ—¥æ”¶ç›˜ä»· (ç”¨äºåˆ¤æ–­å‰ä¸€å¤©æ˜¯å¦ä¸‹è·Œ)
    df["close_prev2"] = df.groupby(level="symbol")["close"].shift(2)
    
    # è¾…åŠ©ï¼šå‰ä¸€æ—¥MA5ã€MA20ã€MA50 (ç”¨äºå›è°ƒä¹°å…¥åˆ¤æ–­)
    df["ma5_prev"] = df.groupby(level="symbol")["ma5"].shift(1)
    df["ma20_prev"] = df.groupby(level="symbol")["ma20"].shift(1)
    df["ma50_prev"] = df.groupby(level="symbol")["ma50"].shift(1)
    
    # è¾…åŠ©ï¼šå‰ä¸€æ—¥ä½ç‚¹ (ç”¨äºPULLBACKæ­¢æŸ)
    df["prev_low"] = df.groupby(level="symbol")["low"].shift(1)
    
    # è¾…åŠ©ï¼šå‰ä¸€æ—¥é«˜ç‚¹ (é¢„ç•™)
    df["prev_high"] = df.groupby(level="symbol")["high"].shift(1)

    # ã€çªç ´ç­–ç•¥è¾…åŠ©å­—æ®µã€‘ä»10å¤©å‰é«˜ç‚¹åˆ°æ˜¨æ—¥çš„æœ€ä½ä»·
    # é€»è¾‘ï¼šä½¿ç”¨ shift(1) åå– rolling(9)ï¼Œå³ T-9 åˆ° T-1 è¿™9å¤©çš„æœ€ä½ä»·
    df["low_since_high_to_yesterday"] = df.groupby(level="symbol")["low"].transform(
        lambda x: x.shift(1).rolling(9, min_periods=5).min()
    )
    
    # ã€çªç ´ç­–ç•¥è¾…åŠ©å­—æ®µã€‘ä»æœ€ä½ç‚¹ä»¥æ¥çš„æœ€é«˜ä»·ï¼ˆç”¨äºåˆ¤æ–­ä»Šå¤©æ˜¯å¦æ˜¯åå¼¹æœ€é«˜ç‚¹ï¼‰
    # é€»è¾‘ï¼šå–æœ€è¿‘10å¤©çš„æœ€é«˜ä»·ï¼ˆåŒ…å«ä»Šå¤©ï¼‰
    df["high_since_low"] = df.groupby(level="symbol")["high"].transform(
        lambda x: x.rolling(10, min_periods=5).max()
    )

    return df


def low_buy_candidates(datas: pd.DataFrame, base_snap: pd.DataFrame) -> pd.DataFrame:
    """
    ã€ä½å¸ä¹°å…¥ç­–ç•¥ã€‘
    é€»è¾‘ï¼š
    1. ç¼©é‡ï¼šæ—¥æ¢æ‰‹ < 30æ—¥å‡å€¼*0.8 ä¸” é‡æ¯” < 1
    2. æ´»è·ƒï¼š60æ—¥æŒ¯å¹… > 50%
    3. ä¼ç¨³ï¼šå½“æ—¥æ¶¨å¹… (-3%, 3%)ï¼Œä¸”æˆäº¤é¢å°äºæ˜¨æ—¥
    4. å½“å¤©ä¸‹è·Œ
    5. å½“å¤©ä»·æ ¼æ¯”60æ—¥é«˜ç‚¹å›æ’¤ > 15%
    """
    # 1. å‡†å¤‡å…¨é‡æ•°æ®è®¡ç®—æŒ‡æ ‡
    df = add_derived_features(datas)
    
    # 2. é”å®šç¬¬ä¸€è½®å…¥é€‰çš„è‚¡ç¥¨
    valid_symbols = base_snap[base_snap["in_pool"]].index.get_level_values("symbol").unique()
    # åªå–è¿™äº›è‚¡ç¥¨çš„æ•°æ®ï¼ˆä¸ºäº†è®¡ç®— rolling å¿…é¡»å–å†å²æ•°æ®ï¼Œä¸èƒ½åªå–æœ€åä¸€å¤©ï¼‰
    sub = df.loc[df.index.get_level_values("symbol").isin(valid_symbols)].copy()

    # 3. è®¡ç®—å½“æ—¥é€»è¾‘
    # å–å‡ºæœ€åä¸€å¤©
    last_day = sub.index.get_level_values("time").max()
    today = sub.xs(last_day, level="time").copy()

    # --- æ¡ä»¶ A: ç¼©é‡ ---
    # æ¢æ‰‹ç‡(äº¤æ˜“é¢) < 30æ—¥å‡å€¼ * 0.8
    cond_turn = today["tr_value"] < (today["avg_tr_value_30"] * 0.8)
    # é‡æ¯” < 1
    today["vol_ratio"] = today["volume"] / today["vol_ma5"]
    cond_volr = today["vol_ratio"] < 1.0

    # --- æ¡ä»¶ B: æ´»è·ƒåº¦ ---
    cond_swing = today["swing_60"] > 0.50

    # --- æ¡ä»¶ C: ä»·æ ¼ä¸æˆäº¤é¢å½¢æ€ ---
    # æ¶¨å¹… (-3%, 3%)
    today["chg"] = (today["close"] / today["close_prev"] - 1.0)
    cond_chg = today["chg"].between(-0.03, 0.03)
    # æˆäº¤é‡ç¼©é‡ (ä»Šå¤© < æ˜¨å¤©) - ä½¿ç”¨ volume æ¯” turnover_value æ›´å‡†ç¡®
    cond_tvo = today["volume"] < today["volume_prev"]

    # --- æ¡ä»¶ D: å½“å¤©ä¸‹è·Œ ---
    cond_down = today["close"] < today["close_prev"]
    
    # --- æ¡ä»¶ E: å½“å¤©ä»·æ ¼æ¯”60æ—¥é«˜ç‚¹å›æ’¤ > 15% ---
    today["drawdown_from_high60"] = (today["high_60"] - today["close"]) / today["high_60"]
    cond_drawdown = today["drawdown_from_high60"] > 0.15
    
    # --- æ¡ä»¶ F: å½“å¤©æ”¶ç›˜ä»·åœ¨5æ—¥å‡çº¿ä¹‹ä¸‹ï¼ˆç¡®ä¿ä»·æ ¼å·²å›è°ƒåˆ°ä½ï¼Œè€Œéé«˜ä½ç¼©é‡ï¼‰---
    cond_below_ma5 = today["close"] < today["ma5"]

    # 4. æ±‡æ€»
    cond_cols = {
        "cond_swing60": cond_swing,
        "cond_turn_shrink": cond_turn,
        "cond_vol_ratio": cond_volr,
        "cond_chg_range": cond_chg,
        "cond_tvo_shrink": cond_tvo,
        "cond_down": cond_down,
        "cond_drawdown_15pct": cond_drawdown,
        "cond_below_ma5": cond_below_ma5
    }
    
    for k, v in cond_cols.items():
        today[k] = v

    today["low_buy_candidate"] = today[list(cond_cols.keys())].all(axis=1)

    out_cols = [
        "close", "chg", "vol_ratio", "swing_60", 
        "ma5", "ma10", "ma20", "tr_value", "high_60", "drawdown_from_high60",
        "volume", "volume_prev",  # æ–°å¢ï¼šç”¨äºè°ƒè¯•æˆäº¤é‡
        *cond_cols.keys(), "low_buy_candidate"
    ]
    return today[out_cols].sort_values("low_buy_candidate", ascending=False)


def breakout_buy_candidates(datas: pd.DataFrame, base_snap: pd.DataFrame) -> pd.DataFrame:
    """
    ã€çªç ´ä¹°å…¥ç­–ç•¥ã€‘
    """
    # 1. åŸºç¡€æŒ‡æ ‡è®¡ç®—
    df = add_derived_features(datas)
    
    # 2. é”å®šç¬¬ä¸€è½®å…¥é€‰çš„è‚¡ç¥¨
    valid_symbols = base_snap[base_snap["in_pool"]].index.get_level_values("symbol").unique()
    sub = df.loc[df.index.get_level_values("symbol").isin(valid_symbols)].copy()

    # ============================================================
    # ============================================================
    
    # A. è®¡ç®—è¿‡å»252å¤©çš„æ»šåŠ¨æœ€å¤§å›æ’¤
    # æ»šåŠ¨æœ€é«˜ä»· (High Watermark)
    sub["roll_max_252"] = sub.groupby(level="symbol")["high"].transform(lambda x: x.rolling(252, min_periods=60).max())
    # æ¯æ—¥å›æ’¤å¹…åº¦
    sub["dd_pct"] = 1.0 - (sub["low"] / sub["roll_max_252"])
    # è¿‡å»252å¤©å†…çš„æœ€å¤§å›æ’¤ (Max Drawdown)
    sub["max_dd_252"] = sub.groupby(level="symbol")["dd_pct"].transform(lambda x: x.rolling(252, min_periods=60).max())

    # B. è®¡ç®—ç›˜æ•´åˆ¤å®šè¾…åŠ©åˆ— (T-1 å’Œ T-11 çš„é•¿æœŸé«˜ç‚¹æ¯”è¾ƒ)
    # é€»è¾‘ï¼šå¦‚æœ T-1 çš„ 252æ—¥é«˜ç‚¹ == T-11 çš„ 252æ—¥é«˜ç‚¹ï¼Œè¯´æ˜æœ€è¿‘10å¤©æ²¡æœ‰åˆ·æ–°é•¿æœŸé«˜ç‚¹ -> ç›˜æ•´ä¸­
    high_252_series = sub["high_252"]
    sub["h252_prev"] = high_252_series.groupby(level="symbol").shift(1)
    sub["h252_prev_11"] = high_252_series.groupby(level="symbol").shift(11)
    
    # C. ã€æ–°å¢ã€‘è®¡ç®—ä»10å¤©å‰é«˜ç‚¹åˆ°æ˜¨æ—¥çš„æœ€ä½ä»·
    # é€»è¾‘ï¼š
    # - high_60_ex10 æ˜¯ T-10 åˆ° T-60 ä¹‹é—´çš„æœ€é«˜ä»·ï¼ˆå³10å¤©ä»¥ä¸Šå‰çš„é«˜ç‚¹ï¼‰
    # - æˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¯¥é«˜ç‚¹ä¹‹ååˆ°æ˜¨æ—¥ä¹‹é—´çš„æœ€ä½ä»·ï¼ˆä¸åŒ…å«ä»Šå¤©ï¼‰
    # - ä½¿ç”¨ shift(1) åå– rolling(9)ï¼Œå³ T-9 åˆ° T-1 è¿™9å¤©çš„æœ€ä½ä»·
    sub["low_since_high_to_yesterday"] = sub.groupby(level="symbol")["low"].transform(
        lambda x: x.shift(1).rolling(9, min_periods=5).min()
    )
    
    # D. ã€æ–°å¢ã€‘è®¡ç®—ä»æœ€ä½ç‚¹ä»¥æ¥çš„æœ€é«˜ä»·ï¼ˆç”¨äºåˆ¤æ–­ä»Šå¤©æ˜¯å¦æ˜¯åå¼¹æœ€é«˜ç‚¹ï¼‰
    # é€»è¾‘ï¼šå–æœ€è¿‘10å¤©çš„æœ€é«˜ä»·ï¼ˆåŒ…å«ä»Šå¤©ï¼‰ï¼Œå¦‚æœä»Šå¤©çš„æ”¶ç›˜ä»·ç­‰äºè¿™ä¸ªæœ€é«˜ä»·ï¼Œè¯´æ˜ä»Šå¤©æ˜¯åå¼¹æœ€é«˜ç‚¹
    sub["high_since_low"] = sub.groupby(level="symbol")["high"].transform(
        lambda x: x.rolling(10, min_periods=5).max()
    )
    
    # ============================================================
    # 3. åˆ‡ç‰‡å–å‡ºâ€œæœ€åä¸€å¤©â€ (æ­¤æ—¶ today å·²åŒ…å«ä¸Šè¿°è®¡ç®—çš„æ–°åˆ—)
    # ============================================================
    last_day = sub.index.get_level_values("time").max()
    today = sub.xs(last_day, level="time").copy()

    # 4. è®¡ç®—å½“æ—¥é€»è¾‘åˆ¤æ–­
    
    # --- æ¡ä»¶ A: æ´»è·ƒä¸ä½ç½® ---
    # å½“å‰ä»·æ ¼æ¥è¿‘ 60æ—¥é«˜ç‚¹ (90%~100%ä¹‹é—´ï¼Œå³è·ç¦»é«˜ç‚¹10%ä»¥å†…ä½†è¿˜æœªçªç ´)
    # ä½¿ç”¨ high_60_ex10ï¼šæ’é™¤æœ€è¿‘10å¤©çš„é«˜ç‚¹ï¼Œç¡®ä¿å–åˆ°çš„é«˜ç‚¹æ˜¯10å¤©å‰å½¢æˆçš„
    cond_near_high = (today["close"] >= (today["high_60_ex10"] * 0.90)) & (today["close"] < today["high_60_ex10"])
    
    # æ–°å¢æ¡ä»¶ï¼šæœ€è¿‘10å¤©å†…ä¸èƒ½åˆ›æ–°é«˜ï¼ˆå³ high_60 â‰ˆ high_60_ex10ï¼Œå…è®¸1%è¯¯å·®ï¼‰
    # å¦‚æœ high_60 > high_60_ex10ï¼Œè¯´æ˜æœ€è¿‘10å¤©åˆ›äº†æ–°é«˜ï¼Œåº”è¯¥æ’é™¤
    cond_no_recent_high = today["high_60"] <= (today["high_60_ex10"] * 1.01)
    
    # ã€æ–°å¢ã€‘ä¸¥æ ¼æ’é™¤å½“å¤©å·²çªç ´çš„è‚¡ç¥¨ï¼šå½“å¤©æœ€é«˜ä»·ä¸èƒ½è¶…è¿‡10å¤©å‰çš„é«˜ç‚¹
    cond_not_breakout_today = today["high"] <= today["high_60_ex10"]

    # --- æ¡ä»¶ B: å›æ’¤ä¸ç›˜æ•´ ---
    # æœ€å¤§å›æ’¤ < 30%
    cond_dd = today["max_dd_252"] < 0.30
    
    # ç›˜æ•´æ—¶é—´ > 10å¤©
    # æ¯”è¾ƒåˆšæ‰ç®—å¥½çš„ shift åˆ—
    # å®¹å·®æ¯”è¾ƒï¼Œé˜²æ­¢æµ®ç‚¹æ•°å¾®å°å·®å¼‚
    consolidation_mask = (today["h252_prev"] <= today["h252_prev_11"] * 1.0001) & \
                         (today["h252_prev"] >= today["h252_prev_11"] * 0.9999)
    
    # å¡«å…… False (é˜²æ­¢åˆšä¸Šå¸‚æ•°æ®ä¸è¶³å¯¼è‡´ NaN)
    cond_consol = consolidation_mask.fillna(False)
    
    # --- æ¡ä»¶ C: ã€æ–°å¢ã€‘ä»10å¤©å‰é«˜ç‚¹åˆ°æ˜¨æ—¥æœ€ä½ç‚¹çš„å›æ’¤åœ¨15%~40%ä¹‹é—´ ---
    # å›æ’¤å¹…åº¦ = (é«˜ç‚¹ - æœ€ä½ç‚¹) / é«˜ç‚¹
    today["drawdown_from_high"] = (today["high_60_ex10"] - today["low_since_high_to_yesterday"]) / today["high_60_ex10"]
    cond_drawdown_range = (today["drawdown_from_high"] >= 0.15) & (today["drawdown_from_high"] <= 0.40)
    
    # --- æ¡ä»¶ D: ã€æ–°å¢ã€‘ä»Šå¤©æ˜¯ä½ç‚¹ä»¥æ¥çš„æœ€é«˜ç‚¹ï¼ˆåå¼¹æœ€é«˜ç‚¹ï¼‰---
    # é€»è¾‘ï¼šä»Šå¤©çš„æœ€é«˜ä»·åº”è¯¥æ˜¯æœ€è¿‘10å¤©å†…çš„æœ€é«˜ä»·ï¼Œè¯´æ˜ä»Šå¤©æ˜¯åå¼¹é˜¶æ®µçš„æœ€å¼ºä¸€å¤©
    # ä½¿ç”¨ high è€Œä¸æ˜¯ closeï¼Œå…è®¸ç›˜ä¸­åˆ›æ–°é«˜çš„æƒ…å†µ
    cond_is_rebound_high = today["high"] >= today["high_since_low"]

    # 5. æ±‡æ€»ç»“æœ
    cond_cols = {
        "cond_near_60d": cond_near_high,  # æ¥è¿‘60æ—¥é«˜ç‚¹
        "cond_no_recent_high": cond_no_recent_high,  # æœ€è¿‘10å¤©æ²¡æœ‰åˆ›æ–°é«˜
        "cond_not_breakout_today": cond_not_breakout_today,  # å½“å¤©æ²¡æœ‰çªç ´
        "cond_drawdown_range": cond_drawdown_range,  # å›æ’¤åœ¨15%~40%ä¹‹é—´
        "cond_is_rebound_high": cond_is_rebound_high  # ä»Šå¤©æ˜¯åå¼¹æœ€é«˜ç‚¹
    }
    
    for k, v in cond_cols.items():
        today[k] = v

    today["breakout_candidate"] = today[list(cond_cols.keys())].all(axis=1)

    # è¾“å‡ºåˆ—
    out_cols = [
        "close", "high", "high_60", "high_60_ex10", "high_252", "swing_60", "max_dd_252",
        "low_since_high_to_yesterday", "drawdown_from_high", "high_since_low",  # è¾“å‡ºæ–°è®¡ç®—çš„åˆ—
        *cond_cols.keys(), "breakout_candidate"
    ]
    
    return today[out_cols].sort_values("breakout_candidate", ascending=False)


def pullback_buy_candidates(datas: pd.DataFrame, base_snap: pd.DataFrame) -> pd.DataFrame:
    """
    ã€å›è°ƒä¹°å…¥ç­–ç•¥ã€‘- å››ç‚¹ç»“æ„
    
    å¯»æ‰¾ç»å…¸çš„"Wåº•å˜å½¢"ç»“æ„ï¼š
    
    ä»·æ ¼
      â”‚     â­ ç‚¹1 (60æ—¥æœ€é«˜ç‚¹)
      â”‚    /\
      â”‚   /  \
      â”‚  /    \        â­ ç‚¹3 (åå¼¹é«˜ç‚¹)
      â”‚ /      \      /\
      â”‚/        \    /  \
      â”‚          \  /    \  â† ç‚¹4åŒºåŸŸï¼ˆä¹°å…¥ç‚¹ï¼‰
      â”‚           \/      \/
      â”‚         â­ ç‚¹2    â­ ç‚¹4
      â”‚        (æœ€ä½ç‚¹)  (æ›´é«˜çš„ä½ç‚¹)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ—¶é—´
    
    æ¡ä»¶ï¼š
    1. ç‚¹1: 60æ—¥å†…çš„æœ€é«˜ç‚¹
    2. ç‚¹2: ç‚¹1ä¹‹åçš„æœ€ä½ç‚¹ï¼Œå›æ’¤ > 20%ï¼ˆç›¸å¯¹äºç‚¹1ï¼‰
    3. ç‚¹3: ç‚¹2ä¹‹åçš„åå¼¹æœ€é«˜ç‚¹ï¼Œåå¼¹ > 15%ï¼ˆç›¸å¯¹äºç‚¹2ï¼‰
    4. ç‚¹4: ç‚¹3ä¹‹åçš„å›è°ƒä½ç‚¹ï¼Œä¸” ç‚¹4 > ç‚¹2ï¼ˆå½¢æˆæ›´é«˜çš„ä½ç‚¹ï¼‰
    5. å½“å‰å¤„äºç‚¹4åŒºåŸŸï¼Œå‡†å¤‡å†æ¬¡ä¸Šæ”»
    """
    # 1. åŸºç¡€æŒ‡æ ‡è®¡ç®—
    df = add_derived_features(datas)
    
    # 2. é”å®šç¬¬ä¸€è½®å…¥é€‰çš„è‚¡ç¥¨
    valid_symbols = base_snap[base_snap["in_pool"]].index.get_level_values("symbol").unique()
    sub = df.loc[df.index.get_level_values("symbol").isin(valid_symbols)].copy()

    # æ–°å¢ï¼šè¡¥å……æ¢æ‰‹ç‡ã€é‡æ¯”ã€æˆäº¤é¢ç¼©é‡ç­‰åˆ—
    # é‡æ¯”åˆ†æ¯ï¼š5æ—¥å‡é‡
    sub["vol_ma5"] = sub.groupby(level="symbol")["volume"].transform(lambda x: x.rolling(5, min_periods=3).mean())
    # æ¢æ‰‹ç‡(äº¤æ˜“é¢/æµé€šå¸‚å€¼)
    denom = sub["float_mktcap"].replace(0, np.nan)
    sub["tr_value"] = sub["turnover_value"] / denom
    # 30æ—¥å¹³å‡æ¢æ‰‹ç‡
    sub["avg_tr_value_30"] = sub.groupby(level="symbol")["tr_value"].transform(lambda x: x.rolling(30, min_periods=15).mean())
    # å‰ä¸€æ—¥æˆäº¤é¢
    sub["turnover_value_prev"] = sub.groupby(level="symbol")["turnover_value"].shift(1)
    # é‡æ¯”
    sub["vol_ratio"] = sub["volume"] / sub["vol_ma5"]
    
    # ============================================================
    # 3. é€è‚¡ç¥¨è®¡ç®—å››ç‚¹ç»“æ„ï¼ˆç²¾ç¡®è®¡ç®—ï¼‰
    # ============================================================
    
    def calc_four_points(group):
        """
        è®¡ç®—å•åªè‚¡ç¥¨çš„å››ç‚¹ç»“æ„
        è¿”å›æœ€åä¸€å¤©çš„å››ç‚¹ä¿¡æ¯
        
        ã€é‡è¦ä¿®æ­£ã€‘ç‚¹4å®šä¹‰ï¼š
        - ç‚¹4å¿…é¡»æ˜¯æœ€åä¸€æ—¥ï¼ˆä¿¡å·æ—¥/æ˜¨å¤©ï¼‰
        - ä¸”æœ€åä¸€æ—¥å¿…é¡»æ˜¯ç‚¹3ä¹‹åçš„æœ€ä½ç‚¹
        - å¦‚æœç‚¹3å’Œæœ€åä¸€æ—¥ä¹‹é—´æœ‰æ›´ä½çš„ç‚¹ï¼Œå›¾å½¢è¢«ç ´åï¼Œä¸æ»¡è¶³æ¡ä»¶
        """
        # åªå–æœ€è¿‘60å¤©
        if len(group) < 30:
            return pd.Series({
                "point1_high": np.nan, "point1_days_ago": np.nan,
                "point2_low": np.nan, "point2_days_ago": np.nan,
                "point3_high": np.nan, "point3_days_ago": np.nan,
                "point4_low": np.nan, "point4_days_ago": np.nan,
                "point4_is_last_day": False,  # æ–°å¢
                "drawdown_p1_p2": np.nan, "rebound_p2_p3": np.nan,
                "drawdown_p3_p4": np.nan, "dist_to_point4": np.nan
            })
        
        recent = group.tail(60).copy()
        recent = recent.reset_index(drop=True)  # ä½¿ç”¨æ•°å­—ç´¢å¼•ä¾¿äºè®¡ç®—
        n = len(recent)
        
        # ç‚¹1: 60æ—¥æœ€é«˜ç‚¹
        p1_idx = recent["high"].idxmax()
        p1_high = recent["high"].iloc[p1_idx]
        p1_days_ago = n - 1 - p1_idx
        
        # ç‚¹1å¿…é¡»åœ¨5å¤©ä»¥ä¸Šå‰
        if p1_days_ago < 5:
            return pd.Series({
                "point1_high": p1_high, "point1_days_ago": p1_days_ago,
                "point2_low": np.nan, "point2_days_ago": np.nan,
                "point3_high": np.nan, "point3_days_ago": np.nan,
                "point4_low": np.nan, "point4_days_ago": np.nan,
                "point4_is_last_day": False,  # æ–°å¢
                "drawdown_p1_p2": np.nan, "rebound_p2_p3": np.nan,
                "drawdown_p3_p4": np.nan, "dist_to_point4": np.nan
            })
        
        # ç‚¹2: ç‚¹1ä¹‹åçš„æœ€ä½ç‚¹
        after_p1 = recent.iloc[p1_idx+1:]
        if len(after_p1) < 3:
            return pd.Series({
                "point1_high": p1_high, "point1_days_ago": p1_days_ago,
                "point2_low": np.nan, "point2_days_ago": np.nan,
                "point3_high": np.nan, "point3_days_ago": np.nan,
                "point4_low": np.nan, "point4_days_ago": np.nan,
                "point4_is_last_day": False,  # æ–°å¢
                "drawdown_p1_p2": np.nan, "rebound_p2_p3": np.nan,
                "drawdown_p3_p4": np.nan, "dist_to_point4": np.nan
            })
        
        p2_idx = after_p1["low"].idxmin()
        p2_low = after_p1["low"].iloc[after_p1.index.get_loc(p2_idx)]
        p2_days_ago = n - 1 - p2_idx
        
        drawdown = (p1_high - p2_low) / p1_high
        
        # ç‚¹2å¿…é¡»åœ¨3å¤©ä»¥ä¸Šå‰ï¼ˆç•™å‡ºåå¼¹ç©ºé—´ï¼‰
        if p2_days_ago < 2:
            return pd.Series({
                "point1_high": p1_high, "point1_days_ago": p1_days_ago,
                "point2_low": p2_low, "point2_days_ago": p2_days_ago,
                "point3_high": np.nan, "point3_days_ago": np.nan,
                "point4_low": np.nan, "point4_days_ago": np.nan,
                "point4_is_last_day": False,  # æ–°å¢
                "drawdown_p1_p2": drawdown, "rebound_p2_p3": np.nan,
                "drawdown_p3_p4": np.nan, "dist_to_point4": np.nan
            })
        
        # ç‚¹3: ç‚¹2ä¹‹åçš„åå¼¹æœ€é«˜ç‚¹
        after_p2 = recent.iloc[p2_idx+1:]
        if len(after_p2) < 2:
            return pd.Series({
                "point1_high": p1_high, "point1_days_ago": p1_days_ago,
                "point2_low": p2_low, "point2_days_ago": p2_days_ago,
                "point3_high": np.nan, "point3_days_ago": np.nan,
                "point4_low": np.nan, "point4_days_ago": np.nan,
                "point4_is_last_day": False,  # æ–°å¢
                "drawdown_p1_p2": drawdown, "rebound_p2_p3": np.nan,
                "drawdown_p3_p4": np.nan, "dist_to_point4": np.nan
            })
        
        p3_idx = after_p2["high"].idxmax()
        p3_high = after_p2["high"].iloc[after_p2.index.get_loc(p3_idx)]
        p3_days_ago = n - 1 - p3_idx
        
        rebound = (p3_high - p2_low) / p2_low
        
        # ç‚¹3å¿…é¡»åœ¨2å¤©ä»¥ä¸Šå‰ï¼ˆç•™å‡ºå›è°ƒç©ºé—´ï¼Œç‚¹3å’Œç‚¹4ä¹‹é—´>=2å¤©ï¼‰
        if p3_days_ago < 2:
            return pd.Series({
                "point1_high": p1_high, "point1_days_ago": p1_days_ago,
                "point2_low": p2_low, "point2_days_ago": p2_days_ago,
                "point3_high": p3_high, "point3_days_ago": p3_days_ago,
                "point4_low": np.nan, "point4_days_ago": np.nan,
                "point4_is_last_day": False,  # æ–°å¢
                "drawdown_p1_p2": drawdown, "rebound_p2_p3": rebound,
                "drawdown_p3_p4": np.nan, "dist_to_point4": np.nan
            })
        
        # ç‚¹4: ã€ä¿®æ­£ã€‘ç‚¹4å¿…é¡»æ˜¯æœ€åä¸€æ—¥ï¼ˆæ˜¨å¤©/ä¿¡å·æ—¥ï¼‰ï¼Œä¸”æ˜¯ç‚¹3ä¹‹åçš„æœ€ä½ç‚¹
        # é€»è¾‘ï¼šé¦–å…ˆæ£€æŸ¥æœ€åä¸€æ—¥æ˜¯å¦æ˜¯ç‚¹3ä¹‹åçš„æœ€ä½ç‚¹
        # å¦‚æœç‚¹3å’Œæœ€åä¸€æ—¥ä¹‹é—´æœ‰æ›´ä½çš„ç‚¹ï¼Œè¯´æ˜å›¾å½¢è¢«ç ´åï¼Œä¸æ»¡è¶³æ¡ä»¶
        after_p3 = recent.iloc[p3_idx+1:]
        if len(after_p3) < 1:
            return pd.Series({
                "point1_high": p1_high, "point1_days_ago": p1_days_ago,
                "point2_low": p2_low, "point2_days_ago": p2_days_ago,
                "point3_high": p3_high, "point3_days_ago": p3_days_ago,
                "point4_low": np.nan, "point4_days_ago": np.nan,
                "point4_is_last_day": False,  # æ–°å¢ï¼šæ ‡è®°ç‚¹4æ˜¯å¦æ˜¯æœ€åä¸€æ—¥
                "drawdown_p1_p2": drawdown, "rebound_p2_p3": rebound,
                "drawdown_p3_p4": np.nan, "dist_to_point4": np.nan
            })
        
        # ã€å…³é”®ä¿®æ”¹ã€‘ç‚¹4å®šä¹‰ä¸ºæœ€åä¸€æ—¥ï¼ˆä¿¡å·æ—¥/æ˜¨å¤©ï¼‰
        # å–æœ€åä¸€æ—¥çš„æœ€ä½ä»·ä½œä¸ºç‚¹4
        last_day_low = recent["low"].iloc[-1]  # æœ€åä¸€æ—¥çš„æœ€ä½ä»·
        
        # æ£€æŸ¥æœ€åä¸€æ—¥æ˜¯å¦æ˜¯ç‚¹3ä¹‹åçš„æœ€ä½ç‚¹
        min_low_after_p3 = after_p3["low"].min()  # ç‚¹3ä¹‹åçš„æœ€ä½ä»·
        point4_is_last_day = (last_day_low <= min_low_after_p3 * 1.001)  # å…è®¸0.1%è¯¯å·®
        
        # ç‚¹4å°±æ˜¯æœ€åä¸€æ—¥
        p4_low = last_day_low
        p4_days_ago = 0  # ç‚¹4å°±æ˜¯æœ€åä¸€æ—¥ï¼Œè·ä»Š0å¤©
        
        # ç‚¹3åˆ°ç‚¹4çš„å›æ’¤å¹…åº¦
        drawdown_p3_p4 = (p3_high - p4_low) / p3_high
        
        # å½“å‰ä»·æ ¼ä¸ç‚¹4çš„è·ç¦»ï¼ˆç‚¹4å°±æ˜¯æœ€åä¸€æ—¥ï¼Œæ‰€ä»¥ç”¨æ”¶ç›˜ä»·ï¼‰
        current_close = recent["close"].iloc[-1]
        dist_to_p4 = (current_close - p4_low) / p4_low
        
        return pd.Series({
            "point1_high": p1_high, "point1_days_ago": p1_days_ago,
            "point2_low": p2_low, "point2_days_ago": p2_days_ago,
            "point3_high": p3_high, "point3_days_ago": p3_days_ago,
            "point4_low": p4_low, "point4_days_ago": p4_days_ago,
            "point4_is_last_day": point4_is_last_day,  # æ–°å¢ï¼šæ ‡è®°ç‚¹4æ˜¯å¦æ˜¯æœ€åä¸€æ—¥çš„æœ€ä½ç‚¹
            "drawdown_p1_p2": drawdown, "rebound_p2_p3": rebound,
            "drawdown_p3_p4": drawdown_p3_p4, "dist_to_point4": dist_to_p4
        })
    
    # å¯¹æ¯åªè‚¡ç¥¨è®¡ç®—å››ç‚¹ç»“æ„
    last_day = sub.index.get_level_values("time").max()
    
    # æŒ‰symbolåˆ†ç»„è®¡ç®—
    results = []
    for symbol in valid_symbols:
        try:
            group = sub.xs(symbol, level="symbol")
            four_points = calc_four_points(group)
            four_points["symbol"] = symbol
            results.append(four_points)
        except Exception as e:
            continue
    
    if not results:
        # è¿”å›ç©ºDataFrame
        return pd.DataFrame()
    
    # åˆå¹¶ç»“æœ
    points_df = pd.DataFrame(results).set_index("symbol")
    
    # è·å–æœ€åä¸€å¤©çš„ä»·æ ¼æ•°æ®
    today = sub.xs(last_day, level="time").copy()

    # åˆå¹¶å››ç‚¹æ•°æ®
    today = today.join(points_df, how="left")

    # æ–°å¢ï¼šå›è°ƒä¹°å…¥é™„åŠ æ¡ä»¶
    # æ¢æ‰‹ç‡(äº¤æ˜“é¢) < 30æ—¥å‡å€¼ * 0.8
    cond_turn = today["tr_value"] < (today["avg_tr_value_30"] * 0.8)
    # é‡æ¯” < 1
    cond_volr = today["vol_ratio"] < 1.0
    # æˆäº¤é‡ç¼©é‡ (ä»Šå¤© < æ˜¨å¤©) - ä½¿ç”¨ volume æ¯” turnover_value æ›´å‡†ç¡®
    cond_tvo = today["volume"] < today["volume_prev"]
    # å‰ä¸€å¤©æ”¶ç›˜ä»·ä½äºå‰ä¸€å¤©çš„MA5ï¼ˆç¡®ä¿ç‚¹4å½“å¤©ä»·æ ¼å·²å›è°ƒåˆ°ä½ï¼‰
    cond_below_ma5 = today["close_prev"] < today["ma5_prev"]
    # å‰ä¸€å¤©æ”¶ç›˜ä»·é«˜äºå‰ä¸€å¤©çš„MA50ï¼ˆç¡®ä¿è¶‹åŠ¿å‘ä¸Šï¼Œæé«˜ç­›é€‰æ ‡å‡†ï¼‰
    cond_above_ma50 = today["close_prev"] > today["ma50_prev"]
    
    # ============================================================
    # 4. æ¡ä»¶åˆ¤æ–­
    # ============================================================
    
    # æ¡ä»¶1: ç‚¹1åœ¨10å¤©ä»¥ä¸Šå‰ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿçš„å›è°ƒç©ºé—´ï¼‰
    cond_point1_timing = today["point1_days_ago"] >= 10
    
    # æ¡ä»¶2: ç‚¹2åœ¨ç‚¹1ä¹‹åï¼ˆç‚¹2è·ä»Šå¤©æ•° < ç‚¹1è·ä»Šå¤©æ•°ï¼‰
    cond_point2_after_point1 = today["point2_days_ago"] < today["point1_days_ago"]
    
    # æ¡ä»¶3: å›æ’¤å¹…åº¦ > 20%
    cond_drawdown = today["drawdown_p1_p2"] >= 0.20
    
    # æ¡ä»¶4: ç‚¹3åœ¨ç‚¹2ä¹‹åï¼ˆç‚¹3è·ä»Šå¤©æ•° < ç‚¹2è·ä»Šå¤©æ•°ï¼‰
    cond_point3_after_point2 = today["point3_days_ago"] < today["point2_days_ago"]
    
    # æ¡ä»¶5: åå¼¹å¹…åº¦ > 15%
    cond_rebound = today["rebound_p2_p3"] >= 0.15
    
    # æ¡ä»¶6: ç‚¹4 > ç‚¹2ï¼ˆå½¢æˆæ›´é«˜çš„ä½ç‚¹ï¼‰
    cond_higher_low = today["point4_low"] > today["point2_low"]
    
    # æ¡ä»¶7: ã€ä¿®æ”¹ã€‘ç‚¹4å¿…é¡»æ˜¯æœ€åä¸€æ—¥ï¼Œä¸”æ˜¯ç‚¹3ä¹‹åçš„æœ€ä½ç‚¹
    # å¦‚æœç‚¹3å’Œæœ€åä¸€æ—¥ä¹‹é—´æœ‰æ›´ä½çš„ç‚¹ï¼Œå›¾å½¢è¢«ç ´åï¼Œä¸æ»¡è¶³æ¡ä»¶
    cond_point4_is_last_day = today["point4_is_last_day"].fillna(False)
    
    # æ¡ä»¶8: ã€åˆ é™¤æ—§çš„"æ¥è¿‘ç‚¹4"æ¡ä»¶ï¼Œå› ä¸ºç‚¹4ç°åœ¨å°±æ˜¯æœ€åä¸€æ—¥ã€‘
    # ä¿ç•™dist_to_point4å­—æ®µç”¨äºå‚è€ƒï¼Œä½†ä¸ä½œä¸ºç­›é€‰æ¡ä»¶
    # cond_near_point4 = today["dist_to_point4"] <= 0.10  # å·²åˆ é™¤
    
    # æ¡ä»¶9: ç‚¹3ä¸èƒ½è¶…è¿‡ç‚¹1ï¼ˆå¦åˆ™å°±æ˜¯æ–°é«˜çªç ´ï¼Œä¸æ˜¯å›è°ƒï¼‰
    cond_point3_below_point1 = today["point3_high"] < today["point1_high"]
    
    # æ¡ä»¶10: å‰ä¸€å¤©ï¼ˆç‚¹4å½“å¤©/æœ€åä¸€æ—¥ï¼‰æ˜¯ä¸‹è·Œçš„
    # ä¿®æ”¹é€»è¾‘ï¼šæ£€æŸ¥å‰ä¸€å¤©æ”¶ç›˜ä»· < å‰ä¸€å¤©çš„å‰ä¸€å¤©æ”¶ç›˜ä»·
    cond_last_day_down = today["close_prev"] < today["close_prev2"]
    
    # æ¡ä»¶11: ç‚¹3å’Œç‚¹4ä¹‹é—´è‡³å°‘é—´éš”2å¤©ï¼ˆç‚¹4æ˜¯æœ€åä¸€æ—¥ï¼Œæ‰€ä»¥å°±æ˜¯ç‚¹3è·ä»Š>=2å¤©ï¼‰
    cond_p3_p4_gap = today["point3_days_ago"] >= 2
    
    # æ¡ä»¶12: ç‚¹3åˆ°ç‚¹4çš„å›æ’¤å¹…åº¦ >= 5%
    cond_p3_p4_drawdown = today["drawdown_p3_p4"] >= 0.05

    # ============================================================
    # 5. æ±‡æ€»ç»“æœ
    # ============================================================
    cond_cols = {
        "cond_point1_timing": cond_point1_timing,           # ç‚¹1åœ¨10å¤©ä»¥ä¸Šå‰
        "cond_point2_after_p1": cond_point2_after_point1,   # ç‚¹2åœ¨ç‚¹1ä¹‹å
        "cond_drawdown_20pct": cond_drawdown,               # å›æ’¤ > 20%
        "cond_point3_after_p2": cond_point3_after_point2,   # ç‚¹3åœ¨ç‚¹2ä¹‹å
        "cond_rebound_15pct": cond_rebound,                 # åå¼¹ > 15%
        "cond_higher_low": cond_higher_low,                 # ç‚¹4 > ç‚¹2
        "cond_point4_is_last_day": cond_point4_is_last_day, # ã€æ–°å¢ã€‘ç‚¹4æ˜¯æœ€åä¸€æ—¥ä¸”æ˜¯ç‚¹3åæœ€ä½ç‚¹
        "cond_p3_below_p1": cond_point3_below_point1,       # ç‚¹3 < ç‚¹1
        "cond_last_day_down": cond_last_day_down,           # æœ€åä¸€å¤©ä¸‹è·Œ
        "cond_p3_p4_gap": cond_p3_p4_gap,                   # ç‚¹3å’Œç‚¹4é—´éš”>=2å¤©
        "cond_p3_p4_drawdown": cond_p3_p4_drawdown,         # ç‚¹3åˆ°ç‚¹4å›æ’¤>=5%
        # æ–°å¢å››é¡¹
        "cond_turn_shrink": cond_turn,
        "cond_vol_ratio": cond_volr,
        "cond_tvo_shrink": cond_tvo,
        "cond_below_ma5": cond_below_ma5,                   # æ”¶ç›˜ä»·ä½äºEMA5
        "cond_above_ma50": cond_above_ma50                  # æ”¶ç›˜ä»·é«˜äºEMA50
    }

    for k, v in cond_cols.items():
        today[k] = v.fillna(False)

    today["pullback_candidate"] = today[list(cond_cols.keys())].all(axis=1)

    # è¾“å‡ºåˆ—
    out_cols = [
        "close", "high", "low",
        "point1_high", "point1_days_ago",
        "point2_low", "point2_days_ago", "drawdown_p1_p2",
        "point3_high", "point3_days_ago", "rebound_p2_p3",
        "point4_low", "point4_days_ago", "point4_is_last_day", "drawdown_p3_p4", "dist_to_point4",  # æ–°å¢ point4_is_last_day
        "tr_value", "avg_tr_value_30", "vol_ratio", "turnover_value", "turnover_value_prev",
        *cond_cols.keys(), "pullback_candidate"
    ]

    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
    for col in out_cols:
        if col not in today.columns:
            today[col] = np.nan

    return today[out_cols].sort_values("pullback_candidate", ascending=False)
# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    symbols = read_stock_list(file_path)
    if not symbols is None:
        # symbol åˆ—è¡¨ï¼Œbp500_list
        datas = yahoo_datas(
            symbols,
            n_days=252,
            use_proxy=True,
            http_proxy="http://127.0.0.1:4780",
            https_proxy="http://127.0.0.1:4780",
            max_workers=4
        )

        # ç¬¬ä¸€è½®ç­›é€‰
        pool = build_base_universe(datas)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š åŸºç¡€é€‰è‚¡æ± ç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"   æ€»è‚¡ç¥¨æ•°: {len(pool)}")
        print(f"   å…¥é€‰è‚¡ç¥¨æ•°: {pool['in_pool'].sum()}")
        print(f"   å…¥é€‰ç‡: {pool['in_pool'].sum()/len(pool)*100:.2f}%")
        print(f"{'='*80}\n")
        
        # æ‰“å°å…¥é€‰çš„è‚¡ç¥¨åˆ—è¡¨
        selected_pool = pool[pool["in_pool"]].copy()
        if len(selected_pool) > 0:
            print(f"\n{'='*80}")
            print(f"âœ… å…¥é€‰åŸºç¡€è‚¡ç¥¨æ± çš„è‚¡ç¥¨ (å…± {len(selected_pool)} åª)")
            print(f"{'='*80}\n")
            
            # é‡ç½®ç´¢å¼•ä»¥ä¾¿æ‰“å°
            selected_display = selected_pool.reset_index()
            
            for idx, row in selected_display.iterrows():
                symbol = row['symbol']
                print(f"\nã€{idx+1}ã€‘ {symbol}")
                print(f"   è‚¡ä»·: ${row['close']:.2f}")
                print(f"   è¡Œä¸š: {row['sector'] if pd.notna(row['sector']) else 'N/A'}")
                print(f"   æ¿å—: {row['industry'] if pd.notna(row['industry']) else 'N/A'}")
                print(f"   æµé€šå¸‚å€¼: ${row['float_mktcap']:,.0f}" if pd.notna(row['float_mktcap']) else "   æµé€šå¸‚å€¼: N/A")
                print(f"   30æ—¥å‡æˆäº¤é¢: ${row['avg_turnover_30']:,.0f}" if pd.notna(row['avg_turnover_30']) else "   30æ—¥å‡æˆäº¤é¢: N/A")
                print(f"   30æ—¥å‡æ¢æ‰‹ç‡: {row['avg_turnover_rate_30']*100:.2f}%" if pd.notna(row['avg_turnover_rate_30']) else "   30æ—¥å‡æ¢æ‰‹ç‡: N/A")
                print(f"   EMA50: ${row['ema50']:.2f}" if pd.notna(row['ema50']) else "   EMA50: N/A")
                print(f"   EMA150: ${row['ema150']:.2f}" if pd.notna(row['ema150']) else "   EMA150: N/A")
                print(f"   æœºæ„æŒè‚¡: {row['institution_pct']*100:.1f}%" if pd.notna(row['institution_pct']) else "   æœºæ„æŒè‚¡: N/A")
                print(f"   å†…éƒ¨æŒè‚¡: {row['insider_pct']*100:.1f}%" if pd.notna(row['insider_pct']) else "   å†…éƒ¨æŒè‚¡: N/A")
            
            print(f"\n{'='*80}")
            print(f"ğŸ“‹ åŸºç¡€è‚¡ç¥¨æ± æ±‡æ€»è¡¨")
            print(f"{'='*80}\n")
            
            # æ‰“å°æ±‡æ€»è¡¨æ ¼
            summary_cols = ["close", "sector", "industry", "float_mktcap", "avg_turnover_30", "avg_turnover_rate_30", "ema50", "ema150"]
            summary_display = selected_display[["symbol"] + summary_cols].copy()
            summary_display["avg_turnover_rate_30"] = summary_display["avg_turnover_rate_30"] * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            print(summary_display.to_string(index=False))
            print(f"\n{'='*80}\n")
        else:
            print("\nâš ï¸ æ²¡æœ‰è‚¡ç¥¨é€šè¿‡åŸºç¡€ç­›é€‰ï¼\n")
        
        # æŸ¥çœ‹è¢«æ·˜æ±°çš„åŸå› ç»Ÿè®¡
        print(f"\n{'='*80}")
        print(f"ğŸ“‰ ç­›é€‰æ¡ä»¶é€šè¿‡ç‡ç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"   ç¾å›½å…¬å¸: {pool['cond_country'].sum()} / {len(pool)} ({pool['cond_country'].sum()/len(pool)*100:.1f}%) ğŸ‡ºğŸ‡¸")
        print(f"   å¸‚å€¼é—¨æ§›: {pool['cond_mktcap'].sum()} / {len(pool)} ({pool['cond_mktcap'].sum()/len(pool)*100:.1f}%)")
        print(f"   æµåŠ¨æ€§: {pool['cond_liq'].sum()} / {len(pool)} ({pool['cond_liq'].sum()/len(pool)*100:.1f}%)")
        print(f"   è‚¡ä»·é—¨æ§›: {pool['cond_price'].sum()} / {len(pool)} ({pool['cond_price'].sum()/len(pool)*100:.1f}%)")
        print(f"   è¡Œä¸šç­›é€‰: {pool['cond_sector'].sum()} / {len(pool)} ({pool['cond_sector'].sum()/len(pool)*100:.1f}%)")
        print(f"   EMAè¶‹åŠ¿: {pool['cond_trend'].sum()} / {len(pool)} ({pool['cond_trend'].sum()/len(pool)*100:.1f}%) â­ å…³é”®")
        print(f"   æ¢æ‰‹ç‡æ´»è·ƒ: {pool['cond_activity'].sum()} / {len(pool)} ({pool['cond_activity'].sum()/len(pool)*100:.1f}%)")
        print(f"{'='*80}\n")
        
        # æŸ¥çœ‹è¢«æ·˜æ±°çš„åŸå› ç¤ºä¾‹
        print(f"\n{'='*80}")
        print(f"âŒ è¢«æ·˜æ±°è‚¡ç¥¨ç¤ºä¾‹ (è¶‹åŠ¿ä¸ç¬¦åˆ)")
        print(f"{'='*80}")
        rejected = pool[~pool["in_pool"] & pool["cond_mktcap"] & ~pool["cond_trend"]]
        if len(rejected) > 0:
            rejected_display = rejected.head(5).reset_index()
            print(rejected_display[["symbol", "close", "ema50", "ema150"]].to_string(index=False))
        else:
            print("æ— ç¤ºä¾‹")
        print(f"{'='*80}\n")

        # å‡è®¾ï¼š
        # datas = è¿™äº›è‚¡ç¥¨çš„å†å²æ•°æ®ï¼ˆ(time,symbol) MultiIndexï¼‰
        # pool  = â€œç¬¬ä¸€è½®å½“æ—¥å¿«ç…§â€ï¼Œç´¢å¼•æ˜¯ (time, symbol)ï¼ŒåŒ…å« in_pool åˆ—

        last_day = pool.index.get_level_values("time").max()
        #print(pool)
        base_snap = pool.xs(last_day, level="time")[["in_pool"]]
        base_snap = base_snap.reset_index()
        base_snap["time"] = last_day
        base_snap = base_snap.set_index(["time","symbol"])[["in_pool"]]
        #print(base_snap)

        # ç¬¬äºŒè½®ç­›é€‰
        low_buy = low_buy_candidates(datas, base_snap)
        breakout_buy = breakout_buy_candidates(datas, base_snap)
        pullback_buy = pullback_buy_candidates(datas, base_snap)  # æ–°å¢ï¼šå›è°ƒä¹°å…¥

        # ç»“æœ
        low_list = low_buy.index[low_buy["low_buy_candidate"]].tolist()
        brk_list = breakout_buy.index[breakout_buy["breakout_candidate"]].tolist()
        pullback_list = pullback_buy.index[pullback_buy["pullback_candidate"]].tolist()  # æ–°å¢
        
        # æ‰“å°è¯¦ç»†çš„ä¹°å…¥å€™é€‰ä¿¡æ¯
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ ä½å¸ä¹°å…¥å€™é€‰ (å…± {len(low_list)} åª)")
        print(f"{'='*80}")
        if low_list:
            for sym in low_list:
                row = low_buy.loc[sym]
                print(f"  ã€{sym}ã€‘ ä¹°å…¥ä»·: ${row['close']:.2f} | MA20: ${row['ma20']:.2f} | å›æ’¤: {row['drawdown_from_high60']*100:.1f}%")
                print(f"        ä»Šæ—¥é‡: {int(row['volume']):,} | æ˜¨æ—¥é‡: {int(row['volume_prev']):,} | é‡æ¯”: {row['vol_ratio']:.2f} | ç¼©é‡: {row['cond_tvo_shrink']}")
        else:
            print("  æ— ")
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ çªç ´ä¹°å…¥å€™é€‰ (å…± {len(brk_list)} åª)")
        print(f"{'='*80}")
        if brk_list:
            for sym in brk_list:
                row = breakout_buy.loc[sym]
                print(f"  ã€{sym}ã€‘ ä¹°å…¥ä»·: ${row['close']:.2f} | çªç ´é«˜ç‚¹: ${row['high_60_ex10']:.2f} | è·é«˜ç‚¹: {(row['high_60_ex10']-row['close'])/row['high_60_ex10']*100:.1f}%")
        else:
            print("  æ— ")
        
        print(f"\n{'='*80}")
        print(f"ğŸ”„ å›è°ƒä¹°å…¥å€™é€‰ (å…± {len(pullback_list)} åª)")
        print(f"{'='*80}")
        if pullback_list:
            for sym in pullback_list:
                row = pullback_buy.loc[sym]
                print(f"  ã€{sym}ã€‘ ä¹°å…¥ä»·: ${row['close']:.2f} | ç‚¹1é«˜: ${row['point1_high']:.2f} | ç‚¹4ä½: ${row['point4_low']:.2f} | è·ç‚¹4: {row['dist_to_point4']*100:.1f}%")
        else:
            print("  æ— ")
        print(f"{'='*80}\n")
        
        # åˆå¹¶ä¿¡å·è¡¨
        signals = (
            low_buy[["low_buy_candidate"]]
            .join(breakout_buy[["breakout_candidate"]], how="outer")
            .join(pullback_buy[["pullback_candidate"]], how="outer")  # æ–°å¢
            .fillna(False)
            .sort_index()
        )

        today_str = datetime.now().strftime("%Y-%m-%d")
        save_file = today_str + '_é€‰è‚¡.json'
        print("ä¿å­˜é€‰è‚¡æ–‡ä»¶...",save_file)
        os.makedirs(save_dir, exist_ok=True)
        signals.to_json(os.path.join(save_dir, save_file),orient='table',indent=2)
